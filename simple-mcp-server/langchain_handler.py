# LangChainå¤„ç†æ¨¡å—
# åŒ…å«æ‰€æœ‰LangChainç›¸å…³çš„å·¥å…·å®šä¹‰å’ŒèŠå¤©å¤„ç†é€»è¾‘

import logging
from typing import Dict, List, Any, Optional
from pydantic import BaseModel
import os
import json
from pathlib import Path

# è®¾ç½®æ—¥å¿—
logger = logging.getLogger("LangChain_Handler")

# æ£€æŸ¥LangChainæ˜¯å¦å¯ç”¨
try:
    from langchain_openai import ChatOpenAI
    from langchain_core.messages import HumanMessage, SystemMessage, AIMessage, ToolMessage
    from langchain_core.prompts import ChatPromptTemplate
    from langchain.agents import create_openai_functions_agent, AgentExecutor
    from langchain.tools import BaseTool
    from langchain_core.tools import tool
    LANGCHAIN_AVAILABLE = True
except ImportError:
    print("âš ï¸  LangChainæœªå®‰è£…ï¼ŒèŠå¤©åŠŸèƒ½å°†ä¸å¯ç”¨")
    print("ğŸ’¡ å®‰è£…å‘½ä»¤: pip install langchain langchain-openai")
    LANGCHAIN_AVAILABLE = False

# å“åº”æ¨¡å‹å®šä¹‰
class ToolCall(BaseModel):
    tool_name: str
    arguments: Dict[str, Any]
    result: Optional[str] = None

class ChatResponse(BaseModel):
    success: bool
    result: Optional[str] = None
    error: Optional[str] = None
    tool_calls: Optional[List[ToolCall]] = None
    model_used: Optional[str] = None

# LangChainå·¥å…·å®šä¹‰ï¼ˆä»…åœ¨LangChainå¯ç”¨æ—¶å®šä¹‰ï¼‰
if LANGCHAIN_AVAILABLE:
    @tool
    def read_file_tool(file_path: str) -> str:
        """è¯»å–æ–‡ä»¶å†…å®¹"""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                return f.read()
        except Exception as e:
            return f"è¯»å–æ–‡ä»¶å¤±è´¥: {str(e)}"
    
    @tool
    def write_file_tool(file_path: str, content: str) -> str:
        """å†™å…¥æ–‡ä»¶å†…å®¹"""
        try:
            with open(file_path, 'w', encoding='utf-8') as f:
                f.write(content)
            return f"æ–‡ä»¶å†™å…¥æˆåŠŸ: {file_path}"
        except Exception as e:
            return f"å†™å…¥æ–‡ä»¶å¤±è´¥: {str(e)}"
    
    @tool
    def list_files_tool(directory_path: str) -> str:
        """åˆ—å‡ºç›®å½•ä¸­çš„æ–‡ä»¶"""
        try:
            files = os.listdir(directory_path)
            return json.dumps(files, ensure_ascii=False, indent=2)
        except Exception as e:
            return f"åˆ—å‡ºæ–‡ä»¶å¤±è´¥: {str(e)}"
    
    # LangChainå·¥å…·åˆ—è¡¨
    LANGCHAIN_TOOLS = [read_file_tool, write_file_tool, list_files_tool]
    
    async def chat_with_langchain(message: str, api_key: str = None, deepseek_api_key: str = None, 
                                model: str = "gpt-3.5-turbo", temperature: float = 0.7, max_tokens: int = 1000) -> dict:
        """ä½¿ç”¨LangChainå¤„ç†èŠå¤©è¯·æ±‚"""
        try:
            logger.info(f"ğŸ¤– å¼€å§‹LangChainèŠå¤©å¤„ç† - æ¨¡å‹: {model}")
            
            # åˆ¤æ–­æ˜¯å¦ä¸ºDeepSeekæ¨¡å‹
            is_deepseek = model.startswith('deepseek')
            
            if is_deepseek and deepseek_api_key:
                # DeepSeekæ¨¡å‹å¤„ç†
                logger.info("ğŸ”¥ ä½¿ç”¨DeepSeekæ¨¡å‹è¿›è¡ŒèŠå¤©")
                
                # é…ç½®DeepSeekæ¨¡å‹
                llm = ChatOpenAI(
                    model=model,
                    api_key=deepseek_api_key,
                    base_url="https://api.deepseek.com",
                    temperature=temperature,
                    max_tokens=max_tokens
                )
                
                # åˆ›å»ºæ¶ˆæ¯
                messages = [
                    SystemMessage(content="ä½ æ˜¯ä¸€ä¸ªæœ‰ç”¨çš„AIåŠ©æ‰‹ï¼Œå¯ä»¥ä½¿ç”¨å·¥å…·æ¥å¸®åŠ©ç”¨æˆ·å®Œæˆä»»åŠ¡ã€‚"),
                    HumanMessage(content=message)
                ]
                
                # ç»‘å®šå·¥å…·åˆ°æ¨¡å‹
                llm_with_tools = llm.bind_tools(LANGCHAIN_TOOLS)
                
                # è°ƒç”¨æ¨¡å‹
                response = await llm_with_tools.ainvoke(messages)
                
                # å¤„ç†å·¥å…·è°ƒç”¨
                tool_calls = []
                if hasattr(response, 'tool_calls') and response.tool_calls:
                    logger.info(f"ğŸ”§ æ£€æµ‹åˆ° {len(response.tool_calls)} ä¸ªå·¥å…·è°ƒç”¨")
                    
                    for tool_call in response.tool_calls:
                        tool_name = tool_call['name']
                        tool_args = tool_call['args']
                        
                        logger.info(f"ğŸ› ï¸  æ‰§è¡Œå·¥å…·: {tool_name}")
                        
                        # æ‰§è¡Œå·¥å…·
                        if tool_name == 'read_file_tool':
                            tool_result = read_file_tool.invoke(tool_args)
                        elif tool_name == 'write_file_tool':
                            tool_result = write_file_tool.invoke(tool_args)
                        elif tool_name == 'list_files_tool':
                            tool_result = list_files_tool.invoke(tool_args)
                        else:
                            tool_result = f"æœªçŸ¥å·¥å…·: {tool_name}"
                        
                        tool_calls.append({
                            "tool_name": tool_name,
                            "arguments": tool_args,
                            "result": str(tool_result)
                        })
                        
                    # æ„å»ºå·¥å…·æ¶ˆæ¯åˆ—è¡¨
                    tool_messages = []
                    for i, tool_call in enumerate(response.tool_calls):
                        # è·å–å¯¹åº”çš„å·¥å…·æ‰§è¡Œç»“æœ
                        if i < len(tool_calls):
                            tool_result = tool_calls[i]["result"]
                            tool_messages.append(ToolMessage(
                                content=str(tool_result),
                                tool_call_id=tool_call['id']
                            ))
                    
                    # å¦‚æœæœ‰å·¥å…·è°ƒç”¨ï¼Œå†æ¬¡è°ƒç”¨æ¨¡å‹è·å–æœ€ç»ˆå“åº”
                    if tool_calls:
                        # æ­£ç¡®çš„æ¶ˆæ¯åºåˆ—ï¼šåŸå§‹æ¶ˆæ¯ + AIå“åº” + å·¥å…·æ¶ˆæ¯
                        final_messages = messages + [response] + tool_messages
                        final_response = await llm.ainvoke(final_messages)
                        result = {
                            "success": True,
                            "error": None,
                            "result": final_response.content,
                            "tool_calls": tool_calls,
                            "model_used": model
                        }
                    else:
                        result = {
                            "success": True,
                            "error": None,
                            "result": response.content,
                            "tool_calls": [],
                            "model_used": model
                        }
                # å¦‚æœæ²¡æœ‰å·¥å…·è°ƒç”¨ï¼Œç›´æ¥è¿”å›å“åº”
                else:
                    result = {
                        "success": True,
                        "error": None,
                        "result": response.content,
                        "tool_calls": [],
                        "model_used": model
                    }
                
                # è¿”å›DeepSeekå¤„ç†ç»“æœ
                return result
                
            else:
                 # OpenAIæ¨¡å‹ä½¿ç”¨Functions Agent
                 system_prompt = ChatPromptTemplate.from_messages([
                     ("system", "ä½ æ˜¯ä¸€ä¸ªæœ‰ç”¨çš„AIåŠ©æ‰‹ï¼Œå¯ä»¥ä½¿ç”¨ä»¥ä¸‹å·¥å…·æ¥å¸®åŠ©ç”¨æˆ·ï¼š\n"
                               "1. read_file: è¯»å–æ–‡ä»¶å†…å®¹\n"
                               "2. write_file: å†™å…¥æ–‡ä»¶å†…å®¹\n"
                               "3. list_files: åˆ—å‡ºç›®å½•ä¸­çš„æ–‡ä»¶\n\n"
                               "å½“ç”¨æˆ·éœ€è¦æ–‡ä»¶æ“ä½œæ—¶ï¼Œè¯·ä¸»åŠ¨ä½¿ç”¨ç›¸åº”çš„å·¥å…·ã€‚"
                               "ä¾‹å¦‚ï¼Œå½“ç”¨æˆ·è¦æ±‚åˆ›å»ºæ–‡ä»¶æ—¶ï¼Œä½¿ç”¨write_fileå·¥å…·ã€‚"),
                     ("human", "{input}"),
                     ("placeholder", "{agent_scratchpad}")
                 ])
                 
                 # é…ç½®OpenAIæ¨¡å‹
                 llm = ChatOpenAI(
                     model=model,
                     api_key=api_key,
                     temperature=temperature,
                     max_tokens=max_tokens
                 )
                 
                 agent = create_openai_functions_agent(llm, LANGCHAIN_TOOLS, system_prompt)
                 agent_executor = AgentExecutor(agent=agent, tools=LANGCHAIN_TOOLS, verbose=True)
                 
                 # æ‰§è¡ŒAgent
                 result = await agent_executor.ainvoke({"input": message})
                 
                 # æå–å·¥å…·è°ƒç”¨ä¿¡æ¯
                 tool_calls = []
                 if 'intermediate_steps' in result:
                     for step in result['intermediate_steps']:
                         if len(step) >= 2:
                             action, observation = step[0], step[1]
                             tool_calls.append({
                                 "tool_name": action.tool,
                                 "arguments": action.tool_input,
                                 "result": str(observation)
                             })
                 
                 return {
                     "success": True,
                     "error": None,
                     "result": result['output'],
                     "tool_calls": tool_calls,
                     "model_used": model
                 }
            
        except Exception as e:
            logger.error(f"âŒ LangChainå¤„ç†é”™è¯¯: {str(e)}")
            return {
                "success": False,
                "error": f"LangChainå¤„ç†é”™è¯¯: {str(e)}",
                "result": None,
                "tool_calls": []
            }
else:
    # LangChainä¸å¯ç”¨æ—¶çš„å ä½ç¬¦å‡½æ•°
    async def chat_with_langchain(message: str, api_key: str, model: str = "gpt-3.5-turbo", 
                                temperature: float = 0.7, max_tokens: int = 1000) -> ChatResponse:
        return ChatResponse(
            success=False,
            error="LangChainæœªå®‰è£…ï¼Œè¯·å…ˆå®‰è£…: pip install langchain langchain-openai"
        )
    
    # ç©ºçš„å·¥å…·åˆ—è¡¨
    LANGCHAIN_TOOLS = []

# å¯¼å‡ºçš„å‡½æ•°å’Œå˜é‡
__all__ = ['chat_with_langchain', 'LANGCHAIN_TOOLS', 'LANGCHAIN_AVAILABLE', 'ChatResponse', 'ToolCall']