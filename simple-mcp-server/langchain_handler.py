#!/usr/bin/env python3
"""
LangChainå¤„ç†å™¨ - é‡æ„ç‰ˆæœ¬
è´Ÿè´£LangChainé›†æˆã€æ¨¡å‹è°ƒç”¨ã€å·¥å…·ç»‘å®šç­‰åŠŸèƒ½
"""

import logging
import asyncio
from typing import Dict, List, Any, Optional, Union
from datetime import datetime

from models import (
    ChatRequest, ChatResponse, ToolCall, ModelConfig, ModelProvider
)
from config import get_model_config, get_config
from tools import tool_manager

# è®¾ç½®æ—¥å¿—
logger = logging.getLogger("LangChain_Handler")

# æ£€æŸ¥LangChainæ˜¯å¦å¯ç”¨
try:
    from langchain_openai import ChatOpenAI
    from langchain_core.messages import HumanMessage, SystemMessage, AIMessage, ToolMessage
    from langchain_core.prompts import ChatPromptTemplate
    from langchain.agents import create_openai_functions_agent, AgentExecutor
    from langchain.tools import BaseTool
    from langchain_core.tools import tool
    LANGCHAIN_AVAILABLE = True
    logger.info("âœ… LangChainå·²åŠ è½½")
except ImportError as e:
    logger.warning(f"âš ï¸ LangChainæœªå®‰è£…: {e}")
    logger.info("ğŸ’¡ å®‰è£…å‘½ä»¤: pip install langchain langchain-openai")
    LANGCHAIN_AVAILABLE = False


class LangChainTool:
    """LangChainå·¥å…·åŒ…è£…å™¨"""

    def __init__(self, tool_name: str, tool_description: str, tool_function):
        """
        åˆå§‹åŒ–LangChainå·¥å…·
        
        Args:
            tool_name: å·¥å…·åç§°
            tool_description: å·¥å…·æè¿°
            tool_function: å·¥å…·å‡½æ•°
        """
        self.name = tool_name
        self.description = tool_description
        self.function = tool_function

    def invoke(self, arguments: Dict[str, Any]) -> str:
        """
        åŒæ­¥è°ƒç”¨å·¥å…·
        
        Args:
            arguments: å·¥å…·å‚æ•°
            
        Returns:
            å·¥å…·æ‰§è¡Œç»“æœ
        """
        try:
            # è·å–æˆ–åˆ›å»ºäº‹ä»¶å¾ªç¯
            try:
                loop = asyncio.get_event_loop()
            except RuntimeError:
                loop = asyncio.new_event_loop()
                asyncio.set_event_loop(loop)

            # è¿è¡Œå¼‚æ­¥å‡½æ•°
            if asyncio.iscoroutinefunction(self.function):
                result = loop.run_until_complete(self.function(**arguments))
            else:
                result = self.function(**arguments)

            return str(result)
        except Exception as e:
            logger.error(f"âŒ LangChainå·¥å…·è°ƒç”¨å¤±è´¥ {self.name}: {e}")
            return f"å·¥å…·è°ƒç”¨å¤±è´¥: {str(e)}"


class ModelClient:
    """æ¨¡å‹å®¢æˆ·ç«¯"""

    def __init__(self, config: ModelConfig):
        """
        åˆå§‹åŒ–æ¨¡å‹å®¢æˆ·ç«¯
        
        Args:
            config: æ¨¡å‹é…ç½®
        """
        self.config = config
        self.client: Optional[ChatOpenAI] = None
        self._initialize_client()

    def _initialize_client(self):
        """åˆå§‹åŒ–æ¨¡å‹å®¢æˆ·ç«¯"""
        if not LANGCHAIN_AVAILABLE:
            raise Exception("LangChainä¸å¯ç”¨")

        if not self.config.api_key:
            raise Exception(f"ç¼ºå°‘ {self.config.provider} APIå¯†é’¥")

        client_params = {
            "model": self.config.model_name,
            "api_key": self.config.api_key,
            "temperature": self.config.temperature,
            "max_tokens": self.config.max_tokens
        }

        if self.config.base_url:
            client_params["base_url"] = self.config.base_url

        self.client = ChatOpenAI(**client_params)
        logger.info(f"âœ… {self.config.provider} å®¢æˆ·ç«¯åˆå§‹åŒ–å®Œæˆ: {self.config.model_name}")

    def bind_tools(self, tools: List[LangChainTool]):
        """
        ç»‘å®šå·¥å…·åˆ°æ¨¡å‹
        
        Args:
            tools: LangChainå·¥å…·åˆ—è¡¨
            
        Returns:
            ç»‘å®šå·¥å…·åçš„æ¨¡å‹
        """
        if not self.client:
            raise Exception("æ¨¡å‹å®¢æˆ·ç«¯æœªåˆå§‹åŒ–")

        # è½¬æ¢ä¸ºLangChainå·¥å…·
        langchain_tools = self._create_langchain_tools(tools)
        return self.client.bind_tools(langchain_tools)

    def _create_langchain_tools(self, tool_wrappers: List[LangChainTool]):
        """
        å°†å·¥å…·åŒ…è£…å™¨è½¬æ¢ä¸ºLangChainå·¥å…·
        
        Args:
            tool_wrappers: LangChainå·¥å…·åŒ…è£…å™¨åˆ—è¡¨
            
        Returns:
            LangChainå·¥å…·åˆ—è¡¨
        """
        langchain_tools = []
        for tool_wrapper in tool_wrappers:
            # åˆ›å»ºå·¥å…·å‡½æ•°ï¼Œä½¿ç”¨é—­åŒ…æ•è·tool_wrapper
            def create_tool_func(wrapper):
                def tool_func(**kwargs):
                    return wrapper.invoke(kwargs)
                # è®¾ç½®å‡½æ•°åå’Œæè¿°
                tool_func.__name__ = wrapper.name
                tool_func.__doc__ = wrapper.description
                return tool_func

            # åˆ›å»ºå‡½æ•°å®ä¾‹
            func = create_tool_func(tool_wrapper)

            # ä½¿ç”¨@toolè£…é¥°å™¨ï¼Œå°è¯•ä¸åŒçš„æ ¼å¼
            try:
                # å°è¯•æ–°ç‰ˆæœ¬çš„è£…é¥°å™¨æ ¼å¼ï¼ˆåªæœ‰æè¿°ï¼‰
                decorated_func = tool(func, description=tool_wrapper.description)
            except TypeError:
                try:
                    # å°è¯•æ›´ç®€å•çš„æ ¼å¼
                    decorated_func = tool(tool_wrapper.description)(func)
                except TypeError:
                    # å¦‚æœéƒ½ä¸è¡Œï¼Œä½¿ç”¨æœ€åŸºæœ¬çš„æ ¼å¼
                    decorated_func = tool(func)

            # ç¡®ä¿å·¥å…·æœ‰æ­£ç¡®çš„åç§°å’Œæè¿°
            decorated_func.name = tool_wrapper.name
            decorated_func.description = tool_wrapper.description
            langchain_tools.append(decorated_func)

        return langchain_tools

    async def ainvoke(self, messages: List[Union[HumanMessage, SystemMessage, AIMessage, ToolMessage]]):
        """
        å¼‚æ­¥è°ƒç”¨æ¨¡å‹
        
        Args:
            messages: æ¶ˆæ¯åˆ—è¡¨
            
        Returns:
            æ¨¡å‹å“åº”
        """
        if not self.client:
            raise Exception("æ¨¡å‹å®¢æˆ·ç«¯æœªåˆå§‹åŒ–")

        return await self.client.ainvoke(messages)


class LangChainHandler:
    """LangChainå¤„ç†å™¨"""

    def __init__(self):
        """åˆå§‹åŒ–LangChainå¤„ç†å™¨"""
        self.clients: Dict[str, ModelClient] = {}
        self.tools: List[LangChainTool] = []
        self._initialize_tools()
        logger.info("ğŸ¤– LangChainå¤„ç†å™¨åˆå§‹åŒ–å®Œæˆ")

    def _initialize_tools(self):
        """åˆå§‹åŒ–å·¥å…·"""
        if not LANGCHAIN_AVAILABLE:
            return

        # ä»å·¥å…·ç®¡ç†å™¨è·å–å¯ç”¨å·¥å…·
        available_tools = tool_manager.list_tools()

        for tool_info in available_tools:
            # åˆ›å»ºå·¥å…·åŒ…è£…å™¨
            async def tool_func(**kwargs):
                from tools import execute_tool
                result = await execute_tool(tool_info.name, kwargs)
                if result.success:
                    return result.result
                else:
                    raise Exception(result.error)

            tool_wrapper = LangChainTool(
                tool_name=tool_info.name,
                tool_description=tool_info.description,
                tool_function=tool_func
            )

            self.tools.append(tool_wrapper)
            logger.info(f"ğŸ”§ æ³¨å†ŒLangChainå·¥å…·: {tool_info.name}")

    def _adapt_tool_arguments(self, tool_name: str, arguments: Dict[str, Any]) -> Dict[str, Any]:
        """
        é€‚é…å·¥å…·å‚æ•°ï¼Œå¤„ç†ä¸åŒæ¨¡å‹å¯èƒ½ä½¿ç”¨çš„å‚æ•°åå·®å¼‚
        
        Args:
            tool_name: å·¥å…·åç§°
            arguments: åŸå§‹å‚æ•°
            
        Returns:
            é€‚é…åçš„å‚æ•°
        """
        # é¦–å…ˆæ£€æŸ¥æ˜¯å¦å‚æ•°è¢«åŒ…è£…åœ¨ kwargs ä¸­
        if isinstance(arguments, dict) and len(arguments) == 1 and 'kwargs' in arguments:
            logger.info(f"ğŸ”„ æ£€æµ‹åˆ° kwargs åŒ…è£…ï¼Œæå–å‚æ•°")
            adapted = arguments['kwargs'].copy()
        else:
            adapted = arguments.copy()

        # write_file å·¥å…·çš„å‚æ•°é€‚é…
        if tool_name == "write_file":
            # å¯èƒ½çš„å‚æ•°åæ˜ å°„
            param_mappings = {
                "path": "file_path",
                "filepath": "file_path",
                "filename": "file_path",
                "file": "file_path",
                "text": "content",
                "data": "content",
                "body": "content"
            }

            for old_key, new_key in param_mappings.items():
                if old_key in adapted and new_key not in adapted:
                    adapted[new_key] = adapted.pop(old_key)
                    logger.info(f"ğŸ”„ å‚æ•°æ˜ å°„: {old_key} -> {new_key}")

        # read_file å·¥å…·çš„å‚æ•°é€‚é…
        elif tool_name == "read_file":
            param_mappings = {
                "path": "file_path",
                "filepath": "file_path",
                "filename": "file_path",
                "file": "file_path"
            }

            for old_key, new_key in param_mappings.items():
                if old_key in adapted and new_key not in adapted:
                    adapted[new_key] = adapted.pop(old_key)
                    logger.info(f"ğŸ”„ å‚æ•°æ˜ å°„: {old_key} -> {new_key}")

        # list_files å·¥å…·çš„å‚æ•°é€‚é…
        elif tool_name == "list_files":
            param_mappings = {
                "path": "directory_path",
                "dir": "directory_path",
                "directory": "directory_path",
                "folder": "directory_path"
            }

            for old_key, new_key in param_mappings.items():
                if old_key in adapted and new_key not in adapted:
                    adapted[new_key] = adapted.pop(old_key)
                    logger.info(f"ğŸ”„ å‚æ•°æ˜ å°„: {old_key} -> {new_key}")

        return adapted

    def _get_or_create_client(self, provider: str, api_key: str = None) -> ModelClient:
        """
        è·å–æˆ–åˆ›å»ºæ¨¡å‹å®¢æˆ·ç«¯
        
        Args:
            provider: æ¨¡å‹æä¾›å•†
            api_key: APIå¯†é’¥ï¼ˆå¯é€‰ï¼Œè¦†ç›–é…ç½®ï¼‰
            
        Returns:
            æ¨¡å‹å®¢æˆ·ç«¯
        """
        # åˆ›å»ºå®¢æˆ·ç«¯é”®
        client_key = f"{provider}_{hash(api_key) if api_key else 'default'}"

        if client_key not in self.clients:
            # è·å–æ¨¡å‹é…ç½®
            config = get_model_config(provider)

            # å¦‚æœæä¾›äº†APIå¯†é’¥ï¼Œè¦†ç›–é…ç½®
            if api_key:
                config.api_key = api_key

            # åˆ›å»ºå®¢æˆ·ç«¯
            self.clients[client_key] = ModelClient(config)

        return self.clients[client_key]

    async def _handle_deepseek_chat(self, request: ChatRequest) -> ChatResponse:
        """
        å¤„ç†DeepSeekæ¨¡å‹èŠå¤©
        
        Args:
            request: èŠå¤©è¯·æ±‚
            
        Returns:
            èŠå¤©å“åº”
        """
        logger.info("ğŸ”¥ ä½¿ç”¨DeepSeekæ¨¡å‹è¿›è¡ŒèŠå¤©")

        # è·å–å®¢æˆ·ç«¯
        client = self._get_or_create_client("deepseek", request.deepseek_api_key)

        # åˆ›å»ºç³»ç»Ÿæ¶ˆæ¯
        system_message = SystemMessage(content=get_config(
            "langchain.system_prompt",
            "ä½ æ˜¯ä¸€ä¸ªæœ‰ç”¨çš„AIåŠ©æ‰‹ï¼Œå¯ä»¥ä½¿ç”¨å·¥å…·æ¥å¸®åŠ©ç”¨æˆ·å®Œæˆä»»åŠ¡ã€‚"
        ))

        # åˆ›å»ºç”¨æˆ·æ¶ˆæ¯
        user_message = HumanMessage(content=request.message)
        messages = [system_message, user_message]

        # ç»‘å®šå·¥å…·
        llm_with_tools = client.bind_tools(self.tools)

        # è°ƒç”¨æ¨¡å‹
        response = await llm_with_tools.ainvoke(messages)

        # å¤„ç†å·¥å…·è°ƒç”¨
        tool_calls = []
        tool_messages = []

        if hasattr(response, 'tool_calls') and response.tool_calls:
            logger.info(f"ğŸ”§ æ£€æµ‹åˆ° {len(response.tool_calls)} ä¸ªå·¥å…·è°ƒç”¨")

            for tool_call in response.tool_calls:
                try:
                    # æå–å·¥å…·ä¿¡æ¯
                    tool_name = tool_call['name']
                    tool_args = tool_call['args']
                    tool_call_id = tool_call['id']

                    logger.info(f"ğŸ› ï¸ æ‰§è¡Œå·¥å…·: {tool_name}")
                    logger.info(f"ğŸ“ å·¥å…·å‚æ•°: {tool_args}")

                    # å‚æ•°é€‚é…å™¨ - å¤„ç†å¯èƒ½çš„å‚æ•°åå·®å¼‚
                    adapted_args = self._adapt_tool_arguments(tool_name, tool_args)
                    if adapted_args != tool_args:
                        logger.info(f"ğŸ”„ å‚æ•°é€‚é…: {tool_args} -> {adapted_args}")

                    # æ‰§è¡Œå·¥å…·
                    from tools import execute_tool
                    tool_result = await execute_tool(tool_name, adapted_args)

                    if tool_result.success:
                        result_text = str(tool_result.result)
                        logger.info(f"âœ… å·¥å…·æ‰§è¡ŒæˆåŠŸ: {tool_name}")
                    else:
                        result_text = f"å·¥å…·æ‰§è¡Œå¤±è´¥: {tool_result.error}"
                        logger.error(f"âŒ å·¥å…·æ‰§è¡Œå¤±è´¥: {tool_name} - {tool_result.error}")

                    # è®°å½•å·¥å…·è°ƒç”¨
                    tool_calls.append(ToolCall(
                        tool_name=tool_name,
                        arguments=tool_args,
                        result=result_text
                    ))

                    # åˆ›å»ºå·¥å…·æ¶ˆæ¯
                    tool_messages.append(ToolMessage(
                        content=result_text,
                        tool_call_id=tool_call_id
                    ))

                except Exception as e:
                    logger.error(f"âŒ å·¥å…·è°ƒç”¨å¼‚å¸¸: {e}")
                    error_message = f"å·¥å…·è°ƒç”¨å¼‚å¸¸: {str(e)}"

                    tool_calls.append(ToolCall(
                        tool_name=tool_call.get('name', 'unknown'),
                        arguments=tool_call.get('args', {}),
                        result=error_message
                    ))

                    tool_messages.append(ToolMessage(
                        content=error_message,
                        tool_call_id=tool_call.get('id', 'unknown')
                    ))

            # å¦‚æœæœ‰å·¥å…·è°ƒç”¨ï¼Œè·å–æœ€ç»ˆå›å¤
            if tool_messages:
                try:
                    final_messages = messages + [response] + tool_messages
                    final_response = await client.ainvoke(final_messages)
                    result_content = final_response.content
                except Exception as e:
                    logger.error(f"âŒ è·å–æœ€ç»ˆå›å¤å¤±è´¥: {e}")
                    # ä½¿ç”¨å·¥å…·æ‰§è¡Œç»“æœä½œä¸ºå›å¤
                    if tool_calls:
                        result_content = f"å·¥å…·æ‰§è¡Œå®Œæˆã€‚{tool_calls[0].result}"
                    else:
                        result_content = "å·¥å…·æ‰§è¡Œå®Œæˆï¼Œä½†æ— æ³•è·å–è¯¦ç»†ç»“æœã€‚"
            else:
                # æ²¡æœ‰å·¥å…·è°ƒç”¨ï¼Œç›´æ¥è¿”å›
                result_content = response.content

        return ChatResponse(
            success=True,
            result=result_content,
            tool_calls=tool_calls,
            model_used=request.model
        )

    async def _handle_openai_chat(self, request: ChatRequest) -> ChatResponse:
        """
        å¤„ç†OpenAIæ¨¡å‹èŠå¤©

        Args:
            request: èŠå¤©è¯·æ±‚

        Returns:
            èŠå¤©å“åº”
        """
        logger.info("ğŸ”„ ä½¿ç”¨OpenAIæ¨¡å‹è¿›è¡ŒèŠå¤©")

        # è·å–å®¢æˆ·ç«¯
        client = self._get_or_create_client("openai", request.api_key)

        # åˆ›å»ºç³»ç»Ÿæç¤º
        system_prompt = ChatPromptTemplate.from_messages([
            ("system", get_config(
                "langchain.openai_system_prompt",
                """ä½ æ˜¯ä¸€ä¸ªæœ‰ç”¨çš„AIåŠ©æ‰‹ï¼Œå¯ä»¥ä½¿ç”¨ä»¥ä¸‹å·¥å…·æ¥å¸®åŠ©ç”¨æˆ·ï¼š
1. read_file: è¯»å–æ–‡ä»¶å†…å®¹
2. write_file: å†™å…¥æ–‡ä»¶å†…å®¹
3. list_files: åˆ—å‡ºç›®å½•ä¸­çš„æ–‡ä»¶

å½“ç”¨æˆ·éœ€è¦æ–‡ä»¶æ“ä½œæ—¶ï¼Œè¯·ä¸»åŠ¨ä½¿ç”¨ç›¸åº”çš„å·¥å…·ã€‚
ä¾‹å¦‚ï¼Œå½“ç”¨æˆ·è¦æ±‚åˆ›å»ºæ–‡ä»¶æ—¶ï¼Œä½¿ç”¨write_fileå·¥å…·ã€‚"""
            )),
            ("human", "{input}"),
            ("placeholder", "{agent_scratchpad}")
        ])

        # åˆ›å»ºAgentï¼ˆéœ€è¦è½¬æ¢å·¥å…·ï¼‰
        langchain_tools = client._create_langchain_tools(self.tools)
        agent = create_openai_functions_agent(client.client, langchain_tools, system_prompt)
        agent_executor = AgentExecutor(agent=agent, tools=langchain_tools, verbose=True)

                 # æ‰§è¡ŒAgent
        result = await agent_executor.ainvoke({"input": request.message})

        # æå–å·¥å…·è°ƒç”¨ä¿¡æ¯
        tool_calls = []
        if 'intermediate_steps' in result:
            for step in result['intermediate_steps']:
                if len(step) >= 2:
                    action, observation = step[0], step[1]
        tool_calls.append(ToolCall(
            tool_name=action.tool,
            arguments=action.tool_input,
            result=str(observation)
        ))

        return ChatResponse(
            success=True,
            result=result['output'],
            tool_calls=tool_calls,
            model_used=request.model
        )

    async def handle_chat(self, request: ChatRequest) -> ChatResponse:
        """
        å¤„ç†èŠå¤©è¯·æ±‚

        Args:
            request: èŠå¤©è¯·æ±‚

        Returns:
            èŠå¤©å“åº”
        """
        if not LANGCHAIN_AVAILABLE:
            return ChatResponse(
                success=False,
                error="LangChainæœªå®‰è£…ï¼Œè¯·å…ˆå®‰è£…: pip install langchain langchain-openai"
            )

        try:
            logger.info(f"ğŸ¤– å¼€å§‹LangChainèŠå¤©å¤„ç† - æ¨¡å‹: {request.model}")
            start_time = datetime.now()

            # åˆ¤æ–­æ¨¡å‹ç±»å‹
            is_deepseek = request.model.startswith('deepseek')

            if is_deepseek and request.deepseek_api_key:
                response = await self._handle_deepseek_chat(request)
            elif not is_deepseek and request.api_key:
                response = await self._handle_openai_chat(request)
            else:
                missing_key = "DeepSeek" if is_deepseek else "OpenAI"
                return ChatResponse(
                    success=False,
                    error=f"ç¼ºå°‘ {missing_key} APIå¯†é’¥"
                )

            # è®¡ç®—å¤„ç†æ—¶é—´
            end_time = datetime.now()
            processing_time = (end_time - start_time).total_seconds()

            logger.info(f"âœ… LangChainå¤„ç†å®Œæˆ - æˆåŠŸ: {response.success}, è€—æ—¶: {processing_time:.3f}ç§’")
            if response.tool_calls:
                logger.info(f"ğŸ”§ å·¥å…·è°ƒç”¨æ¬¡æ•°: {len(response.tool_calls)}")

            return response

        except Exception as e:
            logger.error(f"âŒ LangChainå¤„ç†é”™è¯¯: {str(e)}")
            return ChatResponse(
                success=False,
                error=f"LangChainå¤„ç†é”™è¯¯: {str(e)}"
            )

    def get_available_tools(self) -> List[str]:
        """è·å–å¯ç”¨å·¥å…·åˆ—è¡¨"""
        return [tool.name for tool in self.tools]

    def get_tool_stats(self) -> Dict[str, Any]:
        """è·å–å·¥å…·ç»Ÿè®¡ä¿¡æ¯"""
        from tools import get_tool_stats
        return get_tool_stats()

    def reload_tools(self):
        """é‡æ–°åŠ è½½å·¥å…·"""
        logger.info("ğŸ”„ é‡æ–°åŠ è½½LangChainå·¥å…·...")
        self.tools.clear()
        self._initialize_tools()


# å…¨å±€LangChainå¤„ç†å™¨å®ä¾‹
if LANGCHAIN_AVAILABLE:
    langchain_handler = LangChainHandler()
else:
    langchain_handler = None

# ä¾¿æ·å‡½æ•°
async def handle_chat(request: ChatRequest) -> ChatResponse:
    """å¤„ç†èŠå¤©è¯·æ±‚çš„ä¾¿æ·å‡½æ•°"""
    if langchain_handler:
        return await langchain_handler.handle_chat(request)
    else:
        return ChatResponse(
            success=False,
            error="LangChainä¸å¯ç”¨"
        )

# å…¼å®¹æ€§å‡½æ•°ï¼ˆä¿æŒå‘åå…¼å®¹ï¼‰
async def chat_with_langchain(
    message: str,
    api_key: str = None,
    deepseek_api_key: str = None,
    model: str = "gpt-3.5-turbo",
    temperature: float = 0.7,
    max_tokens: int = 1000
) -> dict:
    """
    å…¼å®¹æ€§å‡½æ•°ï¼šä½¿ç”¨LangChainå¤„ç†èŠå¤©è¯·æ±‚
    
    Args:
        message: ç”¨æˆ·æ¶ˆæ¯
        api_key: OpenAI APIå¯†é’¥
        deepseek_api_key: DeepSeek APIå¯†é’¥
        model: æ¨¡å‹åç§°
        temperature: æ¸©åº¦å‚æ•°
        max_tokens: æœ€å¤§ä»¤ç‰Œæ•°
        
    Returns:
        èŠå¤©å“åº”å­—å…¸
    """
    request = ChatRequest(
        message=message,
        api_key=api_key,
        deepseek_api_key=deepseek_api_key,
        model=model,
        temperature=temperature,
        max_tokens=max_tokens
    )

    response = await handle_chat(request)

    # è½¬æ¢ä¸ºå­—å…¸æ ¼å¼ï¼ˆå‘åå…¼å®¹ï¼‰
    result = {
        "success": response.success,
        "result": response.result,
        "error": response.error,
        "tool_calls": [tc.dict() for tc in response.tool_calls] if response.tool_calls else [],
        "model_used": response.model_used
    }

    return result

# å¯¼å‡º
__all__ = [
    "LangChainHandler", "langchain_handler", "LANGCHAIN_AVAILABLE",
    "handle_chat", "chat_with_langchain"
]
